{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ChemMatData","text":"<p>The <code>ChemMatData</code> project aims to provide easy access to various property prediction datasets from Chemistry and Material Science  to facilitate machine learning applications---with a specific focus on graph neural networks.</p>"},{"location":"first_steps/","title":"First Steps","text":"<p>After installing the <code>chem_mat_data</code> package, it can either be directly imported into a Python code or  be used via the <code>chemdata</code> command line interface:</p> <pre><code>chemdata --help\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-methods","title":"Installation methods","text":"<p>Install the latest release of the package from the PyPi package index or install directly  from the git repository.</p>"},{"location":"installation/#installation-from-pypi","title":"Installation from PyPi","text":"<pre><code>uv pip install chem_mat_data\n</code></pre>"},{"location":"installation/#installing-from-source","title":"Installing from Source","text":"<pre><code>uv pip install git+https://github.com/aimat-lab/chem_mat_data\n</code></pre>"},{"location":"architecture_decisions/001_providing_processed_datasets/","title":"Providing Processed Datasets","text":""},{"location":"architecture_decisions/001_providing_processed_datasets/#status","title":"Status","text":"<p>implemented</p>"},{"location":"architecture_decisions/001_providing_processed_datasets/#context","title":"Context","text":"<p>The <code>chem_mat_data</code> package mainly aims to provide a Python and command line API which can be used to easily  download and access chemistry and material science datasets for machine learning with a specific focus on  graph neural networks. One core question in the design of this package is in which format to provide these  datasets to the end user. This is a tricky matter because the datasets may have slightly different formats  themselves and might even have slighlty different objectives. For example, there can be a distinction between  node-level and graph-level tasks which changes the way in which the ground truth labels have to be provided.  Another difference could be that some datasets may consists of different molecules while other datasets consist  of the same molecule over and over again, only with different geometries.</p>"},{"location":"architecture_decisions/001_providing_processed_datasets/#decision","title":"Decision","text":"<p>The decision made for this package is to provide datasets in two formats simultaneously: - raw. This format should be as close as possible to the original domain-specific representation that the    dataset is provided in. For most datasets this will likely be a CSV file containing the SMILES codes of    different molecules associated with ground truth classification and regression labels. Additionally, this    raw representation could be extended with additional information about the geometric configurations. - processed. In addition to the raw format, each dataset should also be provided in the already processed    format. In this format, the individual elements of the dataset are already present as abstract graph    structures consisting of nodes (atoms) connected by edges (bonds). Additionally each node and edge should    be associated with a numeric feature vector which has been extracted using the Cheminformatics library    <code>RDKit</code>.</p>"},{"location":"architecture_decisions/001_providing_processed_datasets/#consequences","title":"Consequences","text":""},{"location":"architecture_decisions/001_providing_processed_datasets/#advantages","title":"Advantages","text":"<p>Ease of use. The main advantage of providing the processed version of the dataset directly is that this makes  it immensly easy to use the dataset to train a graph neural network. The graph format used in the package is a  generic one and the package specifically aims to provide adapters that transform these generic graph structures  into the necessary object instances required for the most common graph learning libraries such as pytorch geometric  and jraph. This ultimately means that a dataset is training-ready in a couple lines of code.</p> <p>Standardization. The processed graph structures already contain numeric feature vectors for the nodes and edges  which have been obtained from RDKit. If everyone were to use the same processing / featurization pipeline, this would  lead to more comparable results in the end.</p>"},{"location":"architecture_decisions/001_providing_processed_datasets/#disadvantages","title":"Disadvantages","text":"<p>Dataset size. A main disadvantage of the processed datasets is the significantly increased size required  to encode the full graph structure which will affect the download speed and the storage requirements especially  for larger datasets.</p> <p>Rigidity. The processed graph structures already contain numeric feature vectors for the nodes and edges  which have been obtained from RDKit. Besides a standardization effect, this also means that. However, the package also provides the means to implement custom processing on top of the raw dataset  representations if desired.</p>"},{"location":"architecture_decisions/002_custom_graph_represention/","title":"Custom Graph Representation","text":""},{"location":"architecture_decisions/002_custom_graph_represention/#status","title":"Status","text":"<p>implemented</p>"},{"location":"architecture_decisions/002_custom_graph_represention/#context","title":"Context","text":"<p>The package also provides the various datasets to be downloaded in a processed format where the  various molecules are in a generic graph representation. The question that arises in this situation  is how to structure this graph representation, especially since different datasets may have different  requirements.</p> <p>The main requirements for this graph representation are: - support for rich metadata at each level of the graph: node-, edge- and graph-level - support for graph-level feature and label annotations - support for node-level feature vectors - support for edge-level feature vectors</p>"},{"location":"architecture_decisions/002_custom_graph_represention/#decision","title":"Decision","text":"<p>The decision is to use a custom graph representation specifically created by and for this package called  a GraphDict representation. This represents a graph as a simple python native dictionary structure where  special keys identify the various parts of the graph representation. The most important keys are the following:</p> <ul> <li><code>node_indices</code>: The integer indices of the nodes</li> <li><code>node_attributes</code>: A node feature vector for each node in the same order as the node indices</li> <li><code>edge_indices</code>: a list of node index tuples which define which nodes are connected by edges</li> <li><code>edge_attributes</code>: A edge feature vector for each edge in the same order as the edge indices</li> <li><code>graph_labels</code>: A list of label annotations for the graph as a whole</li> </ul> <p>Besides these main keys, the primary feature of the graph dict representation is that it is supposed to be  flexible and that additional properties can be added dynamically by using one of the special prefixes  <code>node_</code>, <code>edge_</code> and <code>graph_</code> to indicate at which level of the graph to attach the information  to.</p>"},{"location":"architecture_decisions/002_custom_graph_represention/#consequences","title":"Consequences","text":""},{"location":"architecture_decisions/002_custom_graph_represention/#advantages","title":"Advantages","text":"<p>Native. The major advantage of this graph representation is that it only relies on python native  datastructures such as dictionaries and lists and by extension is therefore also easily JSON encodable.  Being able to JSON encode is very important for the transfer of the datasets over the internet and  also the potential compatibility with other programming languages when compared to a pickle dump for  example. </p> <p>Generic and Flexible. The flexible format allows to add additional information for datasets that  need it without having to over-complicate the core data structure.</p>"},{"location":"architecture_decisions/002_custom_graph_represention/#disadvantages","title":"Disadvantages","text":"<p>Custom Format. It is a custom format that most people will not be initially familiar with. This is  in contrast to using more common formats such as networkx Graph instances for example, which some  people might already have familarity with. However, we argue that the format is simple enough to  understand and also simple enough to convert to a more common format such as networkx - especially if  the necessary adapaters are provided as part of the package as well.</p>"},{"location":"architecture_decisions/003_local_dataset_cache/","title":"Local Dataset Cache","text":""},{"location":"architecture_decisions/003_local_dataset_cache/#status","title":"Status","text":"<p>implemented</p>"},{"location":"architecture_decisions/003_local_dataset_cache/#context","title":"Context","text":"<p>The ChemMatData package provides the possibility to download graph datasets from a remote file share  server. These datasets can either be in the \"raw\" format - in the case of purely molecular datasets  this usually means a CSV file with the molecule SMILES representations annotated with the target values. But more importantly, these datasets can be downloaded in the already pre-processed format and easily  loaded into the popular graph deep learning libraries.</p> <p>There exists a command line interface and a programming interface to very easily load a dataset with a  single function call. The problem now is that repeated calls to this function would download the dataset  anew each time. This would require a constant internet connection and could take a significant amount of  time for larger datasets.</p>"},{"location":"architecture_decisions/003_local_dataset_cache/#decision","title":"Decision","text":"<p>The mitigate the recurring runtime of re-downloading the datasets each time, a local caching mechanism was  added which was inspired by package managers such as <code>pip</code> which also maintain a similar caching mechanism. Whenever a dataset is downloaded, the downloaded files will be placed into a user-specific caching folder.  For all subsequent retrievals of the dataset, the cached version will be used instead.</p> <p>Inside the cache, each dataset is enumerated by its unique string name and the dataset type (raw or processed).</p>"},{"location":"architecture_decisions/003_local_dataset_cache/#consequences","title":"Consequences","text":""},{"location":"architecture_decisions/003_local_dataset_cache/#advantages","title":"Advantages","text":"<p>Runtime. The clear advantage of a local cache is the runtime. Especially for large datasert and/or repeated  execution of the dataset loading functionality, a local cahce will result in a clear reduction in runtime and  bandwith.</p>"},{"location":"architecture_decisions/003_local_dataset_cache/#disadvantages","title":"Disadvantages","text":"<p>User Storage. For the user, the cache will eat up some storage capacity, which may or may not be significant  depending on the number and size of the stored datasets. Although, this likely won't be a problem as the archived  versions of even the largest datasets currently do now exceed a GB in size.</p> <p>Management Overhead. On the development side, the cache introduces another layer of complexity. Before fetching  the datasets, we need to check if the dataset exists in the cache and after downloading the dataset we need to  add the dataset to the cache.</p>"}]}