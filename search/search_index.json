{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ChemMatData","text":"<p>The <code>chem_mat_data</code> package provides easy access to a large range of property prediction datasets from Chemistry and Material Science.  The aim of this package is to provide the datasets in a unified format suitable to machine learning applications and specifically to train  graph neural networks (GNNs).</p> <p>Specifically, <code>chem_mat_data</code> addresses these aims by providing simple, single-line command line (CLI) and programming (API) interfaces to download  datasets either in raw or in processed (graph) format.</p> <p>Features:</p> <ul> <li>\ud83d\udc0d Easily installable via <code>pip</code></li> <li>\ud83d\udce6 Instant access to a collection of datasets across the domains of chemistry and material science </li> <li>\ud83e\udd16 Direct support of popular graph deep learning libraries like Torch/PyG and Jax/Jraph</li> <li>\ud83e\udd1d Large python version compatibility</li> <li>\u2328\ufe0f Comprehensive command line interface (CLI)</li> </ul>"},{"location":"api_datasets/","title":"Loading Datasets","text":"<p>In <code>chem_mat_data</code>, each dataset is provided in a raw and a processed/graph format.</p> <p>The raw format resembles the format in which the dataset was originally published in and is  usually more compressed and therefore more storage- and bandwidth-efficient to work with. For molecular  datasets, this raw format usually simply consists of a list of SMILES strings that represents different  molecules and their corresponding target value annotations. Since it is more data efficient, this  format is recommended when working with machine learning methods that do not require the full graph  representation, such as methods based on molecular fingerprints.</p> <p>The processed/graph format contains the already pre-processed full graph information for each molecule  in the dataset. This format represents each molecule as a graph structure where all atoms are represented as  graphs and all bonds as the corresponding edges. This format is recommended for machine learning  methods based on graph neural networks (GNNs) since it removes the time-consuming graph pre-processing step.</p>"},{"location":"api_datasets/#loading-raw-datasets","title":"Loading Raw Datasets","text":""},{"location":"api_datasets/#smiles-dataset","title":"SMILES Dataset","text":"<p>For molecular property prediction tasks, the raw dataset format consists of a list of SMILES strings that  represent the various molecules and their corresponding target value annotations.</p> <p>A raw SMILES dataset can be loaded using the <code>load_smiles_dataset</code> function. This function will return  a <code>pandas.DataFrame</code> object that contains a \"smiles\" column as well as various other columns that contain  the target property annotations (whose names differe between the various datasets.)</p> <pre><code>import pandas as pd\nfrom chem_mat_data import load_smiles_dataset\n\ndf: pd.DataFrame = load_smiles_dataset('clintox')\nprint(df.head())\n</code></pre>"},{"location":"api_datasets/#loading-processed-datasets","title":"Loading Processed Datasets","text":"<p>The processed/graph format of a dataset can be loaded with the <code>load_graph_dataset</code> function. This function  will return a list of <code>dict</code> objects which contain various key value pairs that describe the full graph  structure of the molecule. For more information on the structure of these graph representations visis the  Graph Representation documentation.</p> <pre><code>from rich.pretty import pprint\nfrom chem_mat_data import load_graph_dataset\n\ngraphs: list[dict] = load_graph_dataset('clintox')\nprint(f'loaded {len(graphs)} graphs')\nexample_graph = graphs[0]\npprint(example_graph)\n</code></pre>"},{"location":"cli_cache/","title":"<code>cache</code> - Interacting with the Local File System Cache","text":"<p>The <code>chem_mat_data</code> package can be used to download various datasets from a remote file share server.  To make sure that datasets only need to be actually downloaded once, the package uses a local file system  cache. Whenever a dataset is downloaded for the first time, the corresponding files are placed in the cache  and if the same dataset is requested again, it is simply copied from the cache rather than downloaded from  the remote server.</p> <p>The <code>cmdata</code> command line interface provides the <code>cache</code> command group to interact with this cache  folder:</p> <pre><code>cmdata cache --help\n</code></pre>"},{"location":"cli_cache/#viewing-the-cache-content","title":"Viewing the Cache Content","text":"<p>To view all the files that are currently stored in the cache, you can use the <code>cache list</code> command:</p> <pre><code>cmdata cache list\n</code></pre> <p>This will print a list view of all the files that are currently stored in the cache directory, divided  by the raw and processed dataset files.</p>"},{"location":"cli_cache/#resetting-the-cache","title":"Resetting the Cache","text":"<p>If you wich to reset the cache for some reason, you can use the <code>cache clear</code> command:</p> <pre><code>cmdata cache clear\n</code></pre> <p>This command will delete all the files in the cache folder.</p>"},{"location":"cli_config/","title":"<code>config</code> - Interacting with the Local Config File","text":"<p>When using the <code>cmdata</code> CLI, a local <code>config.yml</code> file is automatically created in the user's <code>.config</code>  folder. This config file contains certain options that configure the behavior of the package.</p>"},{"location":"cli_config/#viewing-the-config-file","title":"Viewing the Config File","text":"<p>To view the content of the current config file, you can use the <code>config show</code> command:</p> <pre><code>cmdata config show\n</code></pre>"},{"location":"cli_config/#edit-the-config-file","title":"Edit the Config File","text":"<p>To conveniently edit this config file, you can use the <code>config edit</code> command:</p> <pre><code>cmdata config edit\n</code></pre> <p>This command will open the config file using the system's default text editor.</p>"},{"location":"cli_download/","title":"<code>download</code> - Download Dataset","text":"<p>You can use the <code>download</code> command to download the datasets to the local file system by  supplying the unique string identifier of the corresponding dataset (see List Available Datasets):</p> <pre><code>cmdata download \"clintox\"\n</code></pre> <p>This command will download the raw dataset files into the current working directory (CWD). </p>"},{"location":"cli_download/#changing-the-destination-path","title":"Changing the Destination Path","text":"<p>You can change the destination directory for the download using the <code>--path</code> option:</p> <pre><code>cmdata download --path=\"/tmp\" \"clintox\"\n</code></pre>"},{"location":"cli_download/#downloading-the-graph-dataset","title":"Downloading the Graph Dataset","text":"<p>By default the command downloads the raw version of each dataset. You can supply <code>--full</code> flag  to also download the processed/graph version of the dataset as well:</p> <pre><code>cmdata download --full \"clintox\"\n</code></pre> <p>This command will download the raw <code>clintox.csv</code> file and the <code>clintox.mpack</code> file. The processed  dataset is stored in the MessagePack file format, which is essentially  a binary version of the JSON format. When decoding this file with a programming language of choice, it  will yield a list/sequence of graph dictionaries/objects as described in the Graph Representation documentation.</p>"},{"location":"cli_info/","title":"<code>info</code> - Show Dataset Metadata","text":"<p>You can use the <code>info</code> command to view all the metadata associated with a specific dataset by supplying the  unique string identifier of the corresponding dataset (see List Available Datasets)</p> <pre><code>cmdata info \"clintox\"\n</code></pre> <p>This will print a detail view as shown below. The detailed view about the dataset, for example, contains  information about the number of elements it contains, the type of task it presents (regression/classification)  and the description.</p> <p></p>"},{"location":"cli_list/","title":"<code>list</code> - List Available Datasets","text":"<p>You can use the <code>list</code> command to print a list view of all the datasets which are available on the  remote file share server:</p> <pre><code>cmdata list\n</code></pre> <p>This will create a list view as shown below, where each dataset is identified by a unique string name (first column) and  some additional information.</p> <p></p>"},{"location":"first_steps/","title":"First Steps","text":""},{"location":"first_steps/#command-line-interface-cli","title":"Command Line Interface (CLI)","text":"<p>After installing the <code>chem_mat_data</code> package, you should have access to the <code>cmdata</code> command line interface which  allows to interact with the remote file share server to, for example, list all the available datasets or to download  them into a local directory.</p>"},{"location":"first_steps/#help-option","title":"Help Option","text":"<p>You can use the <code>--help</code> option to get a list of all the available commands:</p> <pre><code>cmdata --help\n</code></pre>"},{"location":"first_steps/#simple-commands","title":"Simple Commands","text":"<p>To view a list of all the available datasets on the remote file share server, you can use the <code>list</code> command:</p> <pre><code>cmdata list\n</code></pre> <p>This command will produce a list view as shown below</p> <p></p> <p>You can then use the <code>download</code> command to download one of the datasets by using its unique string name:</p> <pre><code>cmdata download \"clintox\"\n</code></pre> <p>This command will, for example, download a raw <code>clintox.csv</code> file into the current working directory. This raw  CSV dataset format consists of a list of SMILES molecule representations and their corresponding target value  annotations - in this case a classification label of the molecules toxicity.</p>"},{"location":"first_steps/#programming-interface-api","title":"Programming Interface (API)","text":"<p>Alternatively, the <code>chem_mat_data</code> functionality can be used programmatically as part of python code. The  package provides each dataset either in raw or processed/graph format (For further information on the  distincation visit the Loading Datasets) documentation.</p>"},{"location":"first_steps/#raw-datasets","title":"Raw Datasets","text":"<p>You can use the <code>load_smiles_dataset</code> function to download the raw dataset format. This function will  return the dataset as a <code>pandas.DataFrame</code> object which contains a \"smiles\" column along with the specific  target value annotations as separate data frame columns.</p> <pre><code>import pandas as pd\nfrom chem_mat_data import load_smiles_dataset\n\ndf: pd.DataFrame = load_smiles_dataset('clintox')\nprint(df.head())\n</code></pre>"},{"location":"first_steps/#graph-datasets","title":"Graph Datasets","text":"<p>You can also use the <code>load_graph_dataset</code> function to download the same dataset in the pre-processed graph  representation. This function will return a list of <code>dict</code> objects which contain the full graph representation  of the corresponding molecules.</p> <pre><code>from rich.pretty import pprint\nfrom chem_mat_data import load_graph_dataset\n\ngraphs: list[dict] = load_graph_dataset('clintox')\nexample_graph = graphs[0]\npprint(example_graph)\n</code></pre> <p>For more details on the structure of this graph representation, please refer to the  Graph Representation documentation.</p> <p>(Insert an example of how to train a PyG model with this representation)</p>"},{"location":"graph_representation/","title":"Graph Representation","text":"<p>The <code>chem_mat_data</code> package provides various chemical property prediction datasets in a pre-processed format,  in which molecules are mapped to graph structures that represent the atoms as nodes and the bonds as corresponding  edges.</p>"},{"location":"graph_representation/#graph-dict-structure","title":"Graph Dict Structure","text":"<p>Practically, this graph structure is represented as a native python <code>dict</code> object with a set of specific attributes.</p> <p>Generally, the graph dict structure follows the following naming convention</p> <ul> <li>Prefix <code>node_</code> for node-level attributes with shape \\((V, ?)\\)</li> <li>Prefix <code>edge_</code> for edge-level attributes with shape \\((E, ?)\\)</li> <li>Prefix <code>graph_</code> for graph-level attributes with shape \\((?, )\\)</li> </ul> <p>More specifically, each graph dict object has the following minimal set of attributes:</p> Attribute Description <code>node_indices</code> Integer numpy array of shape \\((V, )\\) where \\(V\\) is the number of nodes in the graph. Contains the unique integer indices of the graph nodes. <code>node_attributes</code> Float numpy array of shape \\((V, N)\\) where \\(V\\) is the number of nodes and \\(N\\) is the number of numeric node features. Contains the numeric feature vectors that represent each node. <code>edge_indices</code> Integer numpy array of shape \\((E, 2)\\) where \\(E\\) is the number of edges in the graph. Contains tuples \\((i, j)\\) of node indices that indicate the existence of an edge between nodes \\(i\\) and \\(j\\). <code>edge_attributes</code> Float numpy array of shape \\((E, M)\\) where \\(E\\) is the number of edges in the graph and \\(M\\) is the number of values in the graph. <code>graph_labels</code> Float numpy array of shape \\((T, )\\) where \\(T\\) is the number of target values associated with each element of the dataset. The target values can either be continuous regression targets such as <code>[1.43, -9.4]</code> or could be one-hot classification labels such as <code>[0, 1, 0]</code>. <code>graph_repr</code> The string numpy array of shape \\((1, )\\) containing the string SMILES representation of the original molecule. <p>Depending on the dataset, the graph dict representation may contain additional attributes that represent custom properties such as the 3D coordinates of nodes/atoms, for exmaple.</p>"},{"location":"installation/","title":"Installation","text":"<p>You can install the latest stable version from the Python Package Index (PyPi) like this:</p> <pre><code>uv pip install chem_mat_data\n</code></pre> <p>Alternatively, you can install the latest development version of the package directly from the Github repository:</p> <pre><code>uv pip install git+https://github.com/aimat-lab/chem_mat_data\n</code></pre> <p>Check your installation. Installing the package should provide access to the <code>cmdata</code> command line interface. You can check this with the following command, which should print the current version of the <code>chem_mat_data</code> package:</p> <pre><code>cmdata --version\n</code></pre>"},{"location":"processing_graphs/","title":"Processing New Graphs","text":"<p>The <code>chem_mat_data</code> package provides a pre-processed graph format in which the individual  molecules of a dataset can directly be loaded in a full graph representation with numeric node and  edge feature vectors to specifically simplify the training of graph neural network (GNN) models.</p> <p>Assuming one trains a GNN model on such a pre-processed dataset, one might also want to use such  a model for inference to predict the target properties of new elements which weren't in the  initial dataset. In such a case, the new elements will have to be processed into the same graph  format to be compatible to be used as input to the trained model.</p> <p>To process new graphs into the same graph format, one can use the package's <code>MoleculeProcessing</code> class.  The <code>process</code> method of such a processing instance takes a molecule SMILES string representation as  an input and returns the corresponding graph dict representation:</p> <pre><code>from rich.pretty import pprint\nfrom chem_mat_data.processing import MoleculeProcessing\n\nprocessing = MoleculeProcessing()\n\nsmiles: str = 'C1=CC=CC=C1CCN'\ngraph: dict = processing.process(smiles)\npprint(graph)\n</code></pre>"},{"location":"architecture_decisions/001_providing_processed_datasets/","title":"Providing Processed Datasets","text":""},{"location":"architecture_decisions/001_providing_processed_datasets/#status","title":"Status","text":"<p>implemented</p>"},{"location":"architecture_decisions/001_providing_processed_datasets/#context","title":"Context","text":"<p>The <code>chem_mat_data</code> package mainly aims to provide a Python and command line API which can be used to easily  download and access chemistry and material science datasets for machine learning with a specific focus on  graph neural networks. One core question in the design of this package is in which format to provide these  datasets to the end user. This is a tricky matter because the datasets may have slightly different formats  themselves and might even have slighlty different objectives. For example, there can be a distinction between  node-level and graph-level tasks which changes the way in which the ground truth labels have to be provided.  Another difference could be that some datasets may consists of different molecules while other datasets consist  of the same molecule over and over again, only with different geometries.</p>"},{"location":"architecture_decisions/001_providing_processed_datasets/#decision","title":"Decision","text":"<p>The decision made for this package is to provide datasets in two formats simultaneously: - raw. This format should be as close as possible to the original domain-specific representation that the    dataset is provided in. For most datasets this will likely be a CSV file containing the SMILES codes of    different molecules associated with ground truth classification and regression labels. Additionally, this    raw representation could be extended with additional information about the geometric configurations. - processed. In addition to the raw format, each dataset should also be provided in the already processed    format. In this format, the individual elements of the dataset are already present as abstract graph    structures consisting of nodes (atoms) connected by edges (bonds). Additionally each node and edge should    be associated with a numeric feature vector which has been extracted using the Cheminformatics library    <code>RDKit</code>.</p>"},{"location":"architecture_decisions/001_providing_processed_datasets/#consequences","title":"Consequences","text":""},{"location":"architecture_decisions/001_providing_processed_datasets/#advantages","title":"Advantages","text":"<p>Ease of use. The main advantage of providing the processed version of the dataset directly is that this makes  it immensly easy to use the dataset to train a graph neural network. The graph format used in the package is a  generic one and the package specifically aims to provide adapters that transform these generic graph structures  into the necessary object instances required for the most common graph learning libraries such as pytorch geometric  and jraph. This ultimately means that a dataset is training-ready in a couple lines of code.</p> <p>Standardization. The processed graph structures already contain numeric feature vectors for the nodes and edges  which have been obtained from RDKit. If everyone were to use the same processing / featurization pipeline, this would  lead to more comparable results in the end.</p>"},{"location":"architecture_decisions/001_providing_processed_datasets/#disadvantages","title":"Disadvantages","text":"<p>Dataset size. A main disadvantage of the processed datasets is the significantly increased size required  to encode the full graph structure which will affect the download speed and the storage requirements especially  for larger datasets.</p> <p>Rigidity. The processed graph structures already contain numeric feature vectors for the nodes and edges  which have been obtained from RDKit. Besides a standardization effect, this also means that. However, the package also provides the means to implement custom processing on top of the raw dataset  representations if desired.</p>"},{"location":"architecture_decisions/002_custom_graph_represention/","title":"Custom Graph Representation","text":""},{"location":"architecture_decisions/002_custom_graph_represention/#status","title":"Status","text":"<p>implemented</p>"},{"location":"architecture_decisions/002_custom_graph_represention/#context","title":"Context","text":"<p>The package also provides the various datasets to be downloaded in a processed format where the  various molecules are in a generic graph representation. The question that arises in this situation  is how to structure this graph representation, especially since different datasets may have different  requirements.</p> <p>The main requirements for this graph representation are: - support for rich metadata at each level of the graph: node-, edge- and graph-level - support for graph-level feature and label annotations - support for node-level feature vectors - support for edge-level feature vectors</p>"},{"location":"architecture_decisions/002_custom_graph_represention/#decision","title":"Decision","text":"<p>The decision is to use a custom graph representation specifically created by and for this package called  a GraphDict representation. This represents a graph as a simple python native dictionary structure where  special keys identify the various parts of the graph representation. The most important keys are the following:</p> <ul> <li><code>node_indices</code>: The integer indices of the nodes</li> <li><code>node_attributes</code>: A node feature vector for each node in the same order as the node indices</li> <li><code>edge_indices</code>: a list of node index tuples which define which nodes are connected by edges</li> <li><code>edge_attributes</code>: A edge feature vector for each edge in the same order as the edge indices</li> <li><code>graph_labels</code>: A list of label annotations for the graph as a whole</li> </ul> <p>Besides these main keys, the primary feature of the graph dict representation is that it is supposed to be  flexible and that additional properties can be added dynamically by using one of the special prefixes  <code>node_</code>, <code>edge_</code> and <code>graph_</code> to indicate at which level of the graph to attach the information  to.</p>"},{"location":"architecture_decisions/002_custom_graph_represention/#consequences","title":"Consequences","text":""},{"location":"architecture_decisions/002_custom_graph_represention/#advantages","title":"Advantages","text":"<p>Native. The major advantage of this graph representation is that it only relies on python native  datastructures such as dictionaries and lists and by extension is therefore also easily JSON encodable.  Being able to JSON encode is very important for the transfer of the datasets over the internet and  also the potential compatibility with other programming languages when compared to a pickle dump for  example. </p> <p>Generic and Flexible. The flexible format allows to add additional information for datasets that  need it without having to over-complicate the core data structure.</p>"},{"location":"architecture_decisions/002_custom_graph_represention/#disadvantages","title":"Disadvantages","text":"<p>Custom Format. It is a custom format that most people will not be initially familiar with. This is  in contrast to using more common formats such as networkx Graph instances for example, which some  people might already have familarity with. However, we argue that the format is simple enough to  understand and also simple enough to convert to a more common format such as networkx - especially if  the necessary adapaters are provided as part of the package as well.</p>"},{"location":"architecture_decisions/003_local_dataset_cache/","title":"Local Dataset Cache","text":""},{"location":"architecture_decisions/003_local_dataset_cache/#status","title":"Status","text":"<p>implemented</p>"},{"location":"architecture_decisions/003_local_dataset_cache/#context","title":"Context","text":"<p>The ChemMatData package provides the possibility to download graph datasets from a remote file share  server. These datasets can either be in the \"raw\" format - in the case of purely molecular datasets  this usually means a CSV file with the molecule SMILES representations annotated with the target values. But more importantly, these datasets can be downloaded in the already pre-processed format and easily  loaded into the popular graph deep learning libraries.</p> <p>There exists a command line interface and a programming interface to very easily load a dataset with a  single function call. The problem now is that repeated calls to this function would download the dataset  anew each time. This would require a constant internet connection and could take a significant amount of  time for larger datasets.</p>"},{"location":"architecture_decisions/003_local_dataset_cache/#decision","title":"Decision","text":"<p>The mitigate the recurring runtime of re-downloading the datasets each time, a local caching mechanism was  added which was inspired by package managers such as <code>pip</code> which also maintain a similar caching mechanism. Whenever a dataset is downloaded, the downloaded files will be placed into a user-specific caching folder.  For all subsequent retrievals of the dataset, the cached version will be used instead.</p> <p>Inside the cache, each dataset is enumerated by its unique string name and the dataset type (raw or processed).</p>"},{"location":"architecture_decisions/003_local_dataset_cache/#consequences","title":"Consequences","text":""},{"location":"architecture_decisions/003_local_dataset_cache/#advantages","title":"Advantages","text":"<p>Runtime. The clear advantage of a local cache is the runtime. Especially for large datasert and/or repeated  execution of the dataset loading functionality, a local cahce will result in a clear reduction in runtime and  bandwith.</p>"},{"location":"architecture_decisions/003_local_dataset_cache/#disadvantages","title":"Disadvantages","text":"<p>User Storage. For the user, the cache will eat up some storage capacity, which may or may not be significant  depending on the number and size of the stored datasets. Although, this likely won't be a problem as the archived  versions of even the largest datasets currently do now exceed a GB in size.</p> <p>Management Overhead. On the development side, the cache introduces another layer of complexity. Before fetching  the datasets, we need to check if the dataset exists in the cache and after downloading the dataset we need to  add the dataset to the cache.</p>"}]}